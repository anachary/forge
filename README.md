# Forge

**AI Coding Agent for VS Code**

*Cursor/Windsurf alternative with multiple AI providers*

Forge is a powerful AI coding agent for VS Code that supports **Claude**, **OpenAI**, **DeepSeek**, and **Ollama** (local). Get the same agentic coding experience as Cursor or Windsurf, with the flexibility to choose your AI provider.

## âœ¨ Features

- **Multiple AI Providers** - Claude, OpenAI, DeepSeek, or fully local with Ollama
- **Agentic Coding** - AI can read, write, and modify files autonomously
- **Tool Execution UI** - Visual feedback for AI actions (like Cursor/Windsurf)
- **Task & Edit Tracking** - See what the AI is working on and what it changed
- **Deep Context** - Semantic code search with local embeddings (sentence-transformers)
- **Call Graph Analysis** - Understands code relationships
- **Git History** - Leverages version control context
- **Web Search** - DuckDuckGo integration (no API key)
- **MCP Support** - Works with Claude Desktop
- **100% Private Option** - Run fully local with Ollama

## ğŸš€ VS Code Extension

The Forge VS Code extension provides a Cursor/Windsurf-like experience right in your editor.

### Supported AI Providers

| Provider | Type | Best For |
|----------|------|----------|
| **Ollama** (default) | Local | Privacy, offline use, free, no API key needed |
| **Claude** | Cloud | Best code understanding & generation |
| **OpenAI** | Cloud | GPT-4o, fast responses |
| **DeepSeek** | Cloud | Cost-effective, great for code |

### Installation

1. Install the extension from the VS Code marketplace (or build from source)
2. Open the Forge panel in the sidebar
3. **Works immediately with Ollama** (default provider, no API key needed)
   - Install [Ollama](https://ollama.ai) and run `ollama serve`
   - The extension defaults to Ollama â€” just start chatting
4. To use a cloud provider instead, open Settings in the Forge panel:
   - **Claude**: Enter your Anthropic API key
   - **OpenAI**: Enter your OpenAI API key
   - **DeepSeek**: Enter your DeepSeek API key
   - API key validation ensures you won't save without a key for cloud providers

### Features

- **Chat Interface** - Natural conversation with your AI
- **Agent Mode** - AI can execute 25 tools to read/write files, run commands, and more
- **Tool Execution Cards** - See each action the AI takes in real-time
- **Agent Summary Bar** - Shows files changed, files examined, and tools used (like Cursor/Windsurf)
- **Logs Tab** - Detailed history of all tool executions
- **Tasks Tab** - Track work items the AI is handling
- **Edits Tab** - Review all file changes made by the AI
- **Thread Management** - Multiple conversation threads with history

### Agent Tools (25 total)

| Category | Tools |
|----------|-------|
| **File Operations** | `read_file`, `write_file`, `list_files`, `rename_file`, `delete_file`, `create_directory` |
| **Code Editing** | `apply_edit` (find & replace), `insert_text` (insert at line) |
| **Code Intelligence** | `go_to_definition`, `find_references`, `get_file_symbols`, `get_hover_info`, `get_diagnostics` |
| **Search** | `search_files` (text/regex across workspace) |
| **Git** | `get_git_status`, `get_git_diff` |
| **Terminal** | `run_command` (execute shell commands) |
| **Web** | `web_search` (DuckDuckGo), `web_fetch` (fetch URL content) |
| **Editor** | `open_file`, `get_open_files`, `show_message` |
| **Workspace** | `get_workspace_info` |
| **Task Tracking** | `add_task`, `update_task` |

## Quick Start

### Prerequisites

1. **Python 3.10+**
2. **Ollama** - Install from [ollama.ai](https://ollama.ai) for local LLM inference (default provider)

### Installation

```bash
# Clone the repository
git clone https://github.com/forge-ai/forge.git
cd forge

# Install dependencies (includes sentence-transformers for local embeddings)
pip install -e .

# Pull the default model for Ollama (default provider)
ollama pull qwen2.5-coder:7b
```

Embeddings work out of the box using `sentence-transformers` (model: `all-MiniLM-L6-v2`) -- no Ollama needed for indexing and search.

### Usage

```bash
# Interactive chat
forge chat

# Index your codebase
forge index

# Search semantically
forge search "authentication logic"

# Run as MCP server
forge mcp
```

## Auto-Indexing

Forge automatically indexes new codebases on first use. **No manual setup required!**

### How It Works

When you open a codebase with Forge:

1. **First Run** ğŸ“¦
   - Forge detects it's a new codebase
   - Automatically indexes all Python files
   - Saves metadata in `.forge/index_metadata.json`
   - Takes 10-30 seconds depending on codebase size

2. **Subsequent Runs** âš¡
   - Detects cached index exists
   - Checks if files have changed
   - If no changes: Uses cached index (instant)
   - If changes detected: Re-indexes automatically

3. **Change Detection** ğŸ”„
   - Tracks file count
   - Monitors last modified timestamps
   - Detects embedding provider changes (auto-clears incompatible vectors)
   - Only re-indexes when needed

### Usage

**Old way (manual):**
```python
from forge.agent import ForgeAgent

agent = ForgeAgent("/path/to/codebase")
agent.initialize()  # Manual call needed
response = agent.chat("What does this code do?")
```

**New way (automatic):**
```python
from forge.agent import ForgeAgent

agent = ForgeAgent("/path/to/codebase")  # Auto-indexes if needed
response = agent.chat("What does this code do?")
```

### Cache Location

Indexing metadata is stored in `.forge/` directory:

```
your-codebase/
â”œâ”€â”€ .forge/
â”‚   â”œâ”€â”€ index_cache              # Marks indexed status
â”‚   â””â”€â”€ index_metadata.json      # File count, timestamps
â””â”€â”€ (your source code)
```

**In `.gitignore`:**
Add `.forge/` to prevent committing cache files.

### Cache Content

`index_metadata.json` tracks:
```json
{
  "file_count": 127,
  "last_modified": 1703830500.245,
  "indexed_at": "2024-12-28T10:15:30.123456",
  "workspace": "/path/to/codebase",
  "embedding_provider": "sentence-transformers"
}
```

### Force Re-Index

If you need to force a full re-index:

```python
from forge.agent import ForgeAgent

agent = ForgeAgent("/path/to/codebase")
agent.initialize(force=True)  # Re-indexes even if cached
```

Or via CLI:

```bash
forge index --force
```

### Performance

| Scenario | Time | Cached? |
|----------|------|---------|
| First run (new codebase) | 10-30s | No |
| Subsequent uses (no changes) | <100ms | Yes âœ… |
| Force re-index | 10-30s | No |
| Large codebase (10K+ files) | 30-120s | After first run |

### What Gets Indexed

- âœ… Python files (`.py`)
- âœ… Code structure (functions, classes)
- âœ… Semantic embeddings
- âœ… Call graph analysis
- âœ… Recent git history

### What Gets Cached

- âœ… Vector embeddings
- âœ… Code chunks
- âœ… Call graph
- âœ… File metadata

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FORGE AGENT                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   User       â”‚â”€â”€â”€â–¶â”‚   Intent     â”‚â”€â”€â”€â–¶â”‚   Context    â”‚      â”‚
â”‚  â”‚   Query      â”‚    â”‚   Classifier â”‚    â”‚   Strategy   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                 â”‚               â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                    â”‚                            â–¼           â”‚   â”‚
â”‚                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚                    â”‚  â”‚       CONTEXT ENGINE            â”‚   â”‚   â”‚
â”‚                    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚   â”‚
â”‚                    â”‚  â”‚                                 â”‚   â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚   â”‚
â”‚  â”‚  Semantic    â”‚â—€â”€â”¼â”€â”€â”¼â”€â”€â”‚ Embedderâ”‚â”€â”€â”€â–¶â”‚ LanceDB  â”‚   â”‚   â”‚   â”‚
â”‚  â”‚  Chunker     â”‚  â”‚  â”‚  â”‚ (local) â”‚    â”‚ (Vector) â”‚   â”‚   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚   â”‚
â”‚        â”‚           â”‚  â”‚                                 â”‚   â”‚   â”‚
â”‚        â–¼           â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚  Call   â”‚    â”‚   Git    â”‚   â”‚   â”‚   â”‚
â”‚  â”‚ Tree-sitter  â”‚  â”‚  â”‚  â”‚  Graph  â”‚    â”‚ Context  â”‚   â”‚   â”‚   â”‚
â”‚  â”‚    AST       â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚                                 â”‚   â”‚   â”‚
â”‚                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚                    â”‚                                        â”‚   â”‚
â”‚                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚                    â”‚  â”‚         TOOLS                   â”‚   â”‚   â”‚
â”‚                    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚   â”‚
â”‚                    â”‚  â”‚  File Ops â”‚ Terminal â”‚ Web      â”‚   â”‚   â”‚
â”‚                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                          â”‚                      â”‚
â”‚                                          â–¼                      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                    â”‚        LLM (Ollama / Claude / OpenAI)    â”‚  â”‚
â”‚                    â”‚       qwen2.5-coder / claude / gpt-4o    â”‚  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                          â”‚                      â”‚
â”‚                                          â–¼                      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                    â”‚            Response + Actions           â”‚  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Concepts

### 1. Retrieval-Augmented Generation (RAG)

Instead of relying solely on the LLM's training data, Forge retrieves relevant code from your codebase and includes it in the prompt.

**How it works:**
1. Code is chunked at semantic boundaries (functions, classes)
2. Each chunk is embedded into a vector using `all-MiniLM-L6-v2` (sentence-transformers, local) or `nomic-embed-text` (Ollama)
3. Vectors are stored in LanceDB (local vector database)
4. User queries are embedded and matched against stored vectors
5. Top matches are included in the LLM prompt

**Reference:**
> Lewis, P., et al. (2020). *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*. NeurIPS.
> [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)

### 2. Semantic Code Chunking

Code is split at meaningful boundaries using AST parsing, not arbitrary character counts.

**Why it matters:**
- Functions stay together
- Classes stay together
- Context is coherent

**Implementation:**
- Uses Tree-sitter for fast, accurate parsing
- Supports Python, JavaScript, TypeScript, Go, Rust, Java

**Reference:**
> Husain, H., et al. (2019). *CodeSearchNet Challenge: Evaluating the State of Semantic Code Search*.
> [arXiv:1909.09436](https://arxiv.org/abs/1909.09436)

### 3. Vector Embeddings

Text is converted to dense vectors where semantic similarity = vector proximity.

**Default Model:** `all-MiniLM-L6-v2` via sentence-transformers
- 384 dimensions
- Fast, good quality for code and text
- Runs locally, no server required

**Alternative:** `nomic-embed-text` via Ollama (768 dimensions, requires Ollama server)

Set via `--embedding-provider ollama` or `FORGE_EMBEDDING_PROVIDER=ollama`.

**Reference:**
> Reimers, N., & Gurevych, I. (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*.
> [arXiv:1908.10084](https://arxiv.org/abs/1908.10084)

### 4. Call Graph Analysis

Static analysis to understand code relationships.

**Enables:**
- "Find all callers of function X"
- "What functions does X call?"
- "What's the impact of changing X?"

**Reference:**
> Horwitz, S., Reps, T., & Binkley, D. (1990). *Interprocedural Slicing Using Dependence Graphs*. ACM TOPLAS.

### 5. Git History Context

Leverages version control for additional signals.

**Provides:**
- Recent changes (what's being worked on)
- File experts (who knows this code)
- Related files (often changed together)

**Reference:**
> Zimmermann, T., et al. (2005). *Mining Version Histories to Guide Software Changes*. IEEE TSE.

### 6. ReAct Pattern (Agentic Behavior)

The agent can reason and act in a loop.

**Pattern:**
1. **Reason** - Think about what to do
2. **Act** - Use a tool
3. **Observe** - See the result
4. **Repeat** - Until task is complete

**Reference:**
> Yao, S., et al. (2022). *ReAct: Synergizing Reasoning and Acting in Language Models*. ICLR.
> [arXiv:2210.03629](https://arxiv.org/abs/2210.03629)

### 7. Intent Classification

Classifies user queries to optimize context retrieval.

| Intent | Codebase | Web | Git |
|--------|----------|-----|-----|
| Explain code | Yes | No | Yes |
| Fix bug | Yes | No | Yes |
| Write code | Yes | No | No |
| Compare tools | Yes | Yes | No |
| External info | No | Yes | No |

### 8. Token Budgeting

Manages context window limits for different models.

| Model Size | Context Budget |
|------------|----------------|
| 7B | ~3,000 tokens |
| 14B+ | ~5,000 tokens |
| Claude/GPT-4 | ~10,000 tokens |

## Project Structure

```
forge/
â”œâ”€â”€ forge/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ cli.py              # Command line interface
â”‚   â”‚
â”‚   â”œâ”€â”€ context/            # Context Engine
â”‚   â”‚   â”œâ”€â”€ embedder.py     # Embeddings (sentence-transformers / Ollama)
â”‚   â”‚   â”œâ”€â”€ chunker.py      # Semantic code chunking
â”‚   â”‚   â”œâ”€â”€ vector_store.py # LanceDB storage
â”‚   â”‚   â”œâ”€â”€ call_graph.py   # Static analysis
â”‚   â”‚   â”œâ”€â”€ git_context.py  # Git history
â”‚   â”‚   â””â”€â”€ retriever.py    # Unified retrieval
â”‚   â”‚
â”‚   â”œâ”€â”€ agent/              # Agent Core
â”‚   â”‚   â”œâ”€â”€ llm.py          # LLM interface
â”‚   â”‚   â”œâ”€â”€ prompt_enhancer.py  # Intent classification
â”‚   â”‚   â””â”€â”€ forge_agent.py  # Main agent
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/              # Agent Tools
â”‚   â”‚   â”œâ”€â”€ file_tools.py   # File operations
â”‚   â”‚   â”œâ”€â”€ terminal.py     # Command execution
â”‚   â”‚   â””â”€â”€ web_search.py   # DuckDuckGo search
â”‚   â”‚
â”‚   â””â”€â”€ mcp/                # MCP Server
â”‚       â””â”€â”€ server.py       # Tool server
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## Configuration

Configuration is managed via environment variables or `~/.forge/config.json`:

```python
# Default configuration (Ollama is the default provider â€” works out of the box)
ForgeConfig(
    provider="ollama",                          # LLM provider: ollama (default) | claude | openai | deepseek
    embedding_provider="sentence-transformers", # Embedding provider: sentence-transformers | ollama
    ollama_url="http://localhost:11434",
    model="qwen2.5-coder:7b",
    embedding_model="nomic-embed-text",         # Auto-selected per provider if not set
    max_context_tokens=4000,
    temperature=0.7,
    enable_web_search=True,
    enable_call_graph=True,
    enable_git_context=True,
)
```

**Note:** Both the CLI (`forge/config.py`) and the VS Code extension (`package.json`) default to Ollama. Cloud providers (Claude, OpenAI, DeepSeek) require an API key â€” the extension validates that a key is provided before saving settings.

### Environment Variables

```bash
FORGE_PROVIDER=ollama                           # LLM provider
FORGE_EMBEDDING_PROVIDER=sentence-transformers  # Embedding provider (default)
FORGE_OLLAMA_URL=http://localhost:11434
FORGE_MODEL=qwen2.5-coder:7b
FORGE_EMBED_MODEL=nomic-embed-text              # Override embedding model
```

## MCP Integration

Forge can run as an MCP (Model Context Protocol) server for use with Claude Desktop or VS Code.

### Claude Desktop

Add to `~/.config/claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "forge": {
      "command": "forge",
      "args": ["mcp"],
      "cwd": "/path/to/your/project"
    }
  }
}
```

### VS Code

The MCP server exposes these tools:
- `read_file` - Read file contents
- `write_file` - Write to files
- `search_codebase` - Semantic search
- `run_command` - Execute commands
- `web_search` - Web search

## Data Flow

```
User Query: "How does the auth middleware work?"
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Intent Classification            â”‚
â”‚    â†’ CODE_EXPLAIN (confidence: 0.9) â”‚
â”‚    â†’ Strategy: codebase + git       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Context Retrieval                â”‚
â”‚    a. Embed query â†’ vector          â”‚
â”‚    b. Search LanceDB â†’ top 5 chunks â”‚
â”‚    c. Get call graph for symbols    â”‚
â”‚    d. Get recent git commits        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Prompt Assembly                  â”‚
â”‚    Context + Query + System Prompt  â”‚
â”‚    (within token budget)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. LLM Generation                   â”‚
â”‚    LLM Provider â†’ selected model     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Tool Execution (if needed)       â”‚
â”‚    ReAct loop until complete        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Response to User
```

## Comparison with Other Tools

| Feature | Forge | Cursor | Windsurf | Copilot |
|---------|-------|--------|----------|---------|
| Claude Support | âœ… | âœ… | âœ… | âŒ |
| OpenAI Support | âœ… | âœ… | âœ… | âœ… |
| DeepSeek Support | âœ… | âŒ | âŒ | âŒ |
| Local LLM (Ollama) | âœ… | âŒ | âŒ | âŒ |
| 100% Private Option | âœ… | âŒ | âŒ | âŒ |
| Agent Tools | 25 | ~15 | ~15 | Limited |
| Tool Execution UI | âœ… | âœ… | âœ… | âŒ |
| Agent Summary Bar | âœ… | âœ… | âœ… | âŒ |
| Code Intelligence | âœ… | âœ… | âœ… | âœ… |
| Semantic Search | âœ… | âœ… | âœ… | Limited |
| Call Graph | âœ… | âŒ | âŒ | âŒ |
| Git Context | âœ… | Limited | Limited | âŒ |
| Web Search | âœ… | âœ… | âœ… | âŒ |
| Web Fetch | âœ… | âŒ | âŒ | âŒ |
| Free | âœ…* | Paid | Paid | Paid |
| Open Source | âœ… | âŒ | âŒ | âŒ |

*Free with Ollama (local). Cloud providers require API keys with usage-based pricing.

## License

MIT License - See LICENSE file for details.

## Contributing

Contributions welcome! Please read CONTRIBUTING.md for guidelines.

## Acknowledgments

Built on the shoulders of giants:
- [sentence-transformers](https://www.sbert.net) - Local embedding models (default)
- [Ollama](https://ollama.ai) - Local LLM runtime
- [LanceDB](https://lancedb.com) - Embedded vector database
- [Tree-sitter](https://tree-sitter.github.io) - Code parsing
- [DuckDuckGo](https://duckduckgo.com) - Privacy-respecting search

